include("layers.jl")
include("multilayernet.jl")

net = MultiLayerNet{Float32}(784, [100, 100, 100], 10);
# @show(net)
# @show softmax(predict(net, Float32[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.06102941176470588,0.24901960784313726,0.24901960784313726,0.24901960784313726,0.24901960784313726,0.24901960784313726,0.12450980392156863,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1230392156862745,0.9343137254901961,1,1,1,1,1,0.9995098039215686,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1230392156862745,0.9975490196078431,1,1,1,1,1,1,1,0.31200980392156863,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.4970588235294118,1,1,0.9997549019607843,0.43676470588235294,0,0.3720588235294118,1,1,0.7497549019607843,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.808578431372549,1,1,0.18725490196078431,0,0,0.06102941176470588,0.9975490196078431,1,0.75,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.9980392156862745,1,0.8122549019607843,0,0,0,0,0.7455882352941177,1,0.75,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.9980392156862745,1,0.5620098039215686,0,0,0,0,0.9955882352941177,1,0.75,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.9980392156862745,1,0.25,0,0,0,0.06200980392156863,0.9990196078431373,1,0.6247549019607843,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.9980392156862745,1,0.25,0,0,0,0.7894607843137255,1,1,0.18725490196078431,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.9980392156862745,1,0.8742647058823529,0,0.1784313725490196,0.44583333333333336,0.9980392156862745,1,0.8125,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.8075980392156863,1,1,0.9083333333333333,0.9985294117647059,1,1,1,0.5620098039215686,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.6183823529411765,0.9997549019607843,1,1,1,1,1,0.9977941176470588,0.12426470588235294,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1230392156862745,0.8088235294117647,0.9997549019607843,1,1,1,1,0.9997549019607843,0.6877450980392157,0.2181372549019608,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.8075980392156863,1,1,1,0.9985294117647059,1,1,1,0.9995098039215686,0.6242647058823529,0.062254901960784315,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.06200980392156863,0.9990196078431373,1,1,0.5620098039215686,0.1857843137254902,0.3730392156862745,0.9360294117647059,1,1,1,0.7495098039215686,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.24803921568627452,1,1,0.5617647058823529,0,0,0,0,0.49607843137254903,1,1,1,0.24975490196078431,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.24803921568627452,1,1,0.6240196078431373,0,0,0,0,0,0.4345588235294118,1,1,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.24803921568627452,1,1,1,0.2178921568627451,0.2409313725490196,0.429656862745098,0.49901960784313726,0.49901960784313726,0.6838235294117647,1,1,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.06102941176470588,0.8090686274509804,1,1,1,1,1,1,1,1,1,1,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.12352941176470589,0.25,0.8948529411764706,1,1,1,1,0.8747549019607843,0.75,0.75,0.6872549019607843,0.12475490196078431,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.15588235294117647,0.2252450980392157,0.24901960784313726,0.25,0.061764705882352944,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]))

function onehot(t::AbstractVector, l::AbstractVector)
    r = zeros(Int, length(l), length(t))
    for i = 1:length(t)
        r[findfirst(l, t[i]), i] = 1
    end
    r
end
function onehot{T}(::Type{T}, t::AbstractVector, l::AbstractVector)
    r = zeros(T, length(l), length(t))
    for i = 1:length(t)
        r[findfirst(l, t[i]), i] = one(T)
    end
    r
end

using MNIST

_x_train, _t_train = traindata();
_x_test, _t_test = testdata();
x_train = collect(Float32, _x_train) ./ 255
t_train = onehot(Float32, _t_train, 0:9)
x_test = collect(Float32, _x_test) ./ 255
t_test = onehot(Float32, _t_test, 0:9)

iters_num = 1000;
train_size = size(x_train, 2); # => 60000
batch_size = 100;
learning_rate = Float32(0.1);

train_loss_list = Float32[];
train_acc_list = Float32[];
test_acc_list = Float32[];

iter_per_epoch = max(train_size ÷ batch_size, 1)

for i = 1:iters_num
    batch_mask = rand(1:train_size, batch_size)
    x_batch = x_train[:, batch_mask]
    t_batch = t_train[:, batch_mask]
    
    # 誤差逆伝播法によって勾配を求める
    grads = gradient(net, x_batch, t_batch)
    
    # 更新
    applygradient!(net, grads, learning_rate)
    
    _loss = loss(net, x_batch, t_batch)
    push!(train_loss_list, _loss)

    if i % iter_per_epoch == 1
        train_acc = accuracy(net, x_train, t_train)
        test_acc = accuracy(net, x_test, t_test)
        push!(train_acc_list, train_acc)
        push!(test_acc_list, test_acc)
        println("$(i-1): train_acc=$(train_acc) / test_acc=$(test_acc)")
    end
end

final_train_acc = accuracy(net, x_train, t_train)
final_test_acc = accuracy(net, x_test, t_test)
push!(train_acc_list, final_train_acc)
push!(test_acc_list, final_test_acc)
println("final: train_acc=$(final_train_acc) / test_acc=$(final_test_acc)")
